# RAG_counselor

## Установка и запуск
Для запуска проекта необходимо в корневой директории прописать следующее:
```bash
docker compose up --build -d
```

PS: Убедитесь, что у вас есть доступ на чтение/запись для файла /var/run/docker.sock

PSS:
В docker-compose.yml для сервисов chroma & rag_service указаны дампнутые вольюмы, лежащие в папке [./volume_dumps/](./volume_dumps). Если хотите, чтобы Airflow самостоятельно собрал инфу, а rag_service создал базу и векторизовал ее, то указывайте вольюмы parse_data, chroma_data для rag_service, chroma соответственно, однако учтите, что векторизация может быть очень долгой по времени.

## Тестирование
#### Немного про запуск тестов для подсчета метрик
Запускать следует из директории ./tests

Внутри есть файл [requirements.txt](./tests/requirements.txt) с описанием необходимых модулей для подсчета метрик.

Установка: 
```bash
python3 -m venv .venv && source ./path/to/.venv/bin/activate && pip install -r requirements.txt
```
### Retrieval часть
* что сделал?
  
  Разделил данные по главам и для каждой главы сделал отдельную коллекцию в chroma (экспериментально было доказано, что это дает существенный выйгрыш по метрике)
  
* Как оценивал:
  
  Пусть есть выдача топ-5 ближайших фрагментов от нашей базы данных. Тогда если в этой выдаче находится релевантный фрагмент, то выдача считается положительной. Общая формула: кол-во положительных выдач / кол-во всех выдач

* Значение метрики = 0.88 

### LLM часть

Сама LLM часть поделена на 2 подзадачи (chain prompting): определение наиболее релевантной главы для запроса (one-shot классификация), формирование финального ответа

$acc_{classifier}\approx0.7$

$acc_{LLM final\ ans}\approx0.63$


  